---
layout: post
title:  "Understanding the Temporal Modeling Approaches for YouTube-8M Challenge"
date:   2018-06-06 
categories: ["ComputerVision"]
author: "Ziyu Zhou"
---

> Get to know the amazing ideas proposed by the thrid place winner of the [Google Cloud & YouTube-8M Video Understanding Challenge](https://www.kaggle.com/c/youtube8m). 
>
> Please refer to their paper [ Temporal Modeling Approaches for Large-scale Youtube-8M Video Understanding](https://arxiv.org/pdf/1707.04555.pdf) and [code](https://github.com/baidu/Youtube-8M).

**Table of Contents:**
<!-- TOC -->

- [What is the YouTube-8M Challenge](#what-is-the-youtube-8m-challenge)
- [Temporal Modeling Approaches](#temporal-modeling-approaches)
    - [Two-stream Sequence Models](#two-stream-sequence-models)
    - [Fast-forward Sequence Models](#fast-forward-sequence-models)
    - [Temporal Residual Neural Networks](#temporal-residual-neural-networks)
- [References](#references)

<!-- /TOC -->

# What is the YouTube-8M Challenge 

[**YouTube-8M**](https://research.google.com/youtube8m/) is a large scale public dataset released by Google containing over 7 million YouTube videos with a vocabulary of 4716 classes, where each video has 1.8 tag classes on average. The task of the YouTube-8M challenge is to develop classification algorithms which accurately assign video-level labels using this dataset.



# Temporal Modeling Approaches 

The third place team proposed three temporal modeling approaches to tackle the task, namely:

* Two-stream Sequence Models
* Fast-forward Sequence Models
* Temporal Residual Neural Networks

In the following sections, you'll get to know:

* Why do they propose this approach? 
* What problems can this approach solve?
* What's the basic architecture?
* What techniques do they use?



## Two-stream Sequence Models

One big problem is how to incorporate visual and audio representations. Their decision was to trained two models separately, one for the RGB visual representation and the other for the audo representation, each obtaining their own feature vectors. Then, the attended features will be concatenated and fed to two fully connected layers and a sigmoid level to perform video-level labeling.

The two models that they used to train the separate features are both bidirectional LSTM or GRU networks. LSTMs and GRUs are good at catching the connections in the time-series data, such as different frames of a video. To remedy the problem of information loss caused by long sequences,  they also added an attention layer after the LSTM or GRU networks.

The figure below shows a simplified pipeline:

![two-stream](/assets/images/posts_imgs/youtube8m/two-stream.png)

The following is the detailed illustration provided by their paper.

![two-stream-full](/assets/images/posts_imgs/youtube8m/two-stream-full.png)



## Fast-forward Sequence Models

What if we want to go deeper? An idea is that deeper network can outperform shallow ones. However, as the depth of the LSMT or GRU networks increases, optimization will become more difficult due to smaller and instable gradient, and the networks will be more vulnerable to overfitting.

The good news is one can add fast-forward connections to the sequence models, which provides a fast path for information to propagate. According to their paper, the fast-forward connections are added in between adjacent LSTM or GRU layers.

The figure below shows a simplified pipeline:

![fast](/assets/images/posts_imgs/youtube8m/fast.png)

The following is the detailed illustration provided by their paper.

![fast-full](/assets/images/posts_imgs/youtube8m/fast-full.png)



## Temporal Residual Neural Networks

Another way to go deeper is to use ResNet (Residual Neural Networks). In the original [ResNet paper](https://arxiv.org/pdf/1512.03385.pdf), the number of layers reaches 152 without overfitting! 

The thrid place team also adapted the idea of ResNet into their approach. According to the paper, they propagate the concatenated data into a Temporal Resnet, which is a stack of 9 Temporal Resnet Blocks (TRB), and each TRB consists of two temporal convolutional layers (followed by batch norm and activation) and a shortcut connection. This Temporal Resnet is used to obtain more discriminative features, which will then be fed into a bi-directional LSTM / GRU model to perform video-level classification.

Take a loot at the architecture they proposed:

![residual](/assets/images/posts_imgs/youtube8m/residual.png)



# References

[1] <https://arxiv.org/pdf/1707.04555.pdf>

[2] <https://github.com/baidu/Youtube-8M>

[3] <https://arxiv.org/pdf/1512.03385.pdf>

[4] <http://colah.github.io/posts/2015-08-Understanding-LSTMs/>

[5] <https://arxiv.org/pdf/1606.04199.pdf>

